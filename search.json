[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "mlsquare was started around 2018 as an open source iniative to develop ML techniques to help understand, develop, and improve in a holistic sense.\nIt was argued in the position paper that one has to go beyond model-centric ML and performance-centric ML, and should embrace holistic ML that includes aspects as interoperability, explainability, uncertainty quantification, efficient training, among others. Due to the scale, opacity, complexity and cost of developing modern ML models (implying deep learning models), one has to take assistance of other ML models. For example, one ML model can be explain the predictions of another ML model, so on and so forth. ML for ML is a recurrent theme in the problems being tackled (hence the name ML square)\nAs a concrete measure, transpilation and model distillation techniques were developed to port models in one framework (say sklearn) into another (say PyTorch). This work was published in the 1st AI/ML Systems conference [ paper, code ].\nFast forward to the post-GPT world, while building MVPs with LLMs like chatGPT became much more easier, building reliable and trustworthy LLM-based AI products are much harder. So multiple models working together (LLM for LLM) to address a complex set of challenges (like evaluating an LLM, for eg.) is becoming more prevalent and relevant in this post-GPT, multi-model, multi-agentic paradigm. Further, the entire supply-chain of manufacturing these LLMs requires massive concentration of money, might (talent) and muscle (compute). As a result, building local, customized SLMs/LLMs in vernacular languages is extremely hard.\nThese challenges were addressed FedEm from a pedagogic point of view but lot more needs be done. Therefore, the next attempts will be about creating an alternative build process to develop LLMs/SLMs. But the goal remains the same â€“ democratize AI/ML for people, by people, with ML-for-ML.\nYours openly\nThe Saddle Point\n\nFollow on X/Twitter\nFollow on LinkedIn\nJoin the Discord server here"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "mlsquare",
    "section": "",
    "text": "mlsquare is an open source, deep tech initiative to democratize ML. The fundamental tenet is, use ML to make ML useful."
  },
  {
    "objectID": "index.html#past-ml-software-projects",
    "href": "index.html#past-ml-software-projects",
    "title": "mlsquare",
    "section": "Past ML software projects",
    "text": "Past ML software projects\n\nmlsquare: make ML interoperable (paper, code)"
  },
  {
    "objectID": "index.html#wip-ml-software-projects",
    "href": "index.html#wip-ml-software-projects",
    "title": "mlsquare",
    "section": "WIP ML software projects",
    "text": "WIP ML software projects\n\nFedEm: a framework to decentralize the development of LLMs"
  },
  {
    "objectID": "index.html#upcoming-ml-software-projects",
    "href": "index.html#upcoming-ml-software-projects",
    "title": "mlsquare",
    "section": "Upcoming ML software projects",
    "text": "Upcoming ML software projects\n\nneural tokenizers to enable the extension of English-centric LLMs to other languages\nmodel grafting via analytical and numerical approximations to enable efficient cross-architecture transfer learning\ncompute & data efficient training via layer-by-layer and block-by-block LLM training, which is inherently embarrassingly parallel, and with potential analytical update rules (no need for backprop and gradients)\ndeep kernel machines a framework for composing many transformer-like architectures with specifiable inductive biases\nxKANs a framework for composing many KAN-like architectures with specifiable non-parametric function approximators like cubic B-splines, Chebyshev polynomials, Wavelets, etc.."
  },
  {
    "objectID": "index.html#upcoming-courses",
    "href": "index.html#upcoming-courses",
    "title": "mlsquare",
    "section": "Upcoming Courses",
    "text": "Upcoming Courses\n\nMLOps: Theory and Practice for upper level undergraduate students\nTheory of Deep Learning: Why it works - A Constructionist Approach for upper level undergraduate students\n\nYours openly\nThe Saddle Point\n\nFollow on X/Twitter\nFollow on LinkedIn\nJoin the Discord server here"
  }
]